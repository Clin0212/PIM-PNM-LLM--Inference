# PIM-PNM-LLM-Inference


## Abstract
TBD

## Table of Contents
Links to conference/journal publications in PIM/PNM of LLM inference.



### 1. Existing Survey

- `2024/01` | `Survey`|`Arxiv 2024`| The Landscape of Compute-near-memory and Compute-in-memory: A Research and Commercial Overview
\-
[[Paper](https://arxiv.org/pdf/2401.14428)]
    - Not LLM-specific.
- `2024/06` | `Survey`|`Arxiv 2024`| Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference \-
[[Paper](https://arxiv.org/pdf/2406.08413)]

### 2. Technology: PIM or PNM
#### 2.1 PIM （Processing-In-Memory）
- | `SeoulU`|`HPCA 2024`| Unleashing the Potential of PIM: Accelerating Large Batched Inference of Transformer-Based Generative Models.
- |`ASPLOS 2024`|AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference
- |`ASPLOS 2024`| NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing
- |`ASPLOS 2024`| IANUS: Integrated Accelerator based on NPU-PIM Unified Memory System

- |`Arxiv 2023`|PIM-GPT: A Hybrid Process-in-Memory Accelerator for Autoregressive Transformers

- |`Arxiv 2024`|SAL-PIM: A Subarray-level Processing-in-Memory Architecture with LUT-based Linear Interpolation for Transformer-based Text Generation 
- |`MLSys 2024`|JIT-Q: Just-in-time Quantization with Processing-In-Memory for Efficient ML Training

#### 2.2 PNM （Processing-Near-Memory）
- | `SeoulU`|`HPCA 2024`| Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System
- | `Samsung`|`HPCA 2024`|An LPDDR-based CXL-PNM Platform for TCO-efficient Inference of Transformer-based Large Language Models
- |`UCSD` | ` ISCA 2024`| Enabling Efficient Large Recommendation Model Training with Near CXL Memory Processing
- `2024` | ` ISCA 2024`| MECLA: Memory-Compute-Efficient LLM Accelerator with Scaling Sub-matrix Partition

### 3. Benchmark （LLM Inference, Recommender System，Graph Processing）


#### 3.2 Recommender
- `2024` | ` ISCA 2024`| Enabling Efficient Large Recommendation Model Training with Near CXL Memory Processing

### 4. Industry V.S. Academic
#### 4.1
- | `Samsung`|`IEEE Micro 2024`|The Breakthrough Memory Solutions for Improved Performance on LLM Inference
- | `Samsung`|`HPCA 2024`|An LPDDR-based CXL-PNM Platform for TCO-efficient Inference of Transformer-based Large Language Models

### 5. Platform (DDR type)

### 6. Complexity of computation / storage units

### 7. Software-level programming framework (model splitting and task scheduling)
- `2023` | ` SOSP 2023`| Efficient Memory Management for Large Language Model Serving with PagedAttention

- `2024` | ` PPoPP 2024`| LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization

- `2024` | ` Arxiv 2024`| AttentionLego: An Open-Source Building Block For Spatially-Scalable Large Language Model Accelerator With Processing-In-Memory Technology

### 8. System scalability (memory v.s. computing)



### 2. unknown
- `2024` | ` IEEE Micro 2024`| The Breakthrough Memory Solutions for Improved Performance on LLM Inference 
\-
[[Paper](https://ieeexplore.ieee.org/document/10477465)]




- `2024/01` | `Apple`|`Arxiv 2024`| Llm in a flash: Efficient large language model inference with limited memory
\-
[[Paper](https://arxiv.org/pdf/2312.11514)]








- `2024` | ` FPGA 2024`| FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs
\-
[[Paper](https://arxiv.org/pdf/2401.03868)]









- `2024` | ` Arxiv 2024`| Efficient Sparse Processing-in-Memory Architecture (ESPIM) for Machine Learning Inference

