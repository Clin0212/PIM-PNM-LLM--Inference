# PIM-PNM-LLM-Inference


## Abstract
TBD

## Table of Contents
Links to conference/journal publications in PIM/PNM of LLM inference.



### 1. Existing Survey

- `2024/01` | `Survey`|`Arxiv 2024`| The Landscape of Compute-near-memory and Compute-in-memory: A Research and Commercial Overview
\-
[[Paper](https://arxiv.org/pdf/2401.14428)]
    - Not LLM-specific.


### 2. unknown
- `2024` | ` IEEE Micro 2024`| The Breakthrough Memory Solutions for Improved Performance on LLM Inference 
\-
[[Paper](https://ieeexplore.ieee.org/document/10477465)]




- `2024/01` | `Apple`|`Arxiv 2024`| Llm in a flash: Efficient large language model inference with limited memory
\-
[[Paper](https://arxiv.org/pdf/2312.11514)]

- `2024` | ` ISCA 2024`| Enabling Efficient Large Recommendation Model Training with Near CXL Memory Processing

- `2024` | ` ISCA 2024`| MECLA: Memory-Compute-Efficient LLM Accelerator with Scaling Sub-matrix Partition

- `2023` | ` SOSP 2023`| Efficient Memory Management for Large Language Model Serving with PagedAttention


- `2024` | ` FPGA 2024`| FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs
\-
[[Paper](https://arxiv.org/pdf/2401.03868)]



- `2024` | ` PPoPP 2024`| LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization


- `2024` | ` Arxiv 2024`| AttentionLego: An Open-Source Building Block For Spatially-Scalable Large Language Model Accelerator With Processing-In-Memory Technology


- `2024` | ` Arxiv 2024`| Efficient Sparse Processing-in-Memory Architecture (ESPIM) for Machine Learning Inference

